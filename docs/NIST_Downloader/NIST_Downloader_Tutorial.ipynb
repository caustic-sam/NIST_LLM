{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d76e4cca",
   "metadata": {},
   "source": [
    "# üï∏Ô∏è NIST PDF Scraper: SP800 Downloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10eb106f",
   "metadata": {},
   "source": [
    "This notebook walks through scraping the NIST SP800 series for PDF links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c6d138",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743b36ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "NIST PDF Scraper: Crawl, Extract Links, and Download PDFs from the NIST SP800 Publications\n",
    "\n",
    "This script:\n",
    "1. Scrapes the NIST Special Publications (SP800) webpage.\n",
    "2. Finds links to individual publication pages.\n",
    "3. Extracts direct links to PDF files.\n",
    "4. Downloads each PDF to the local directory.\n",
    "\n",
    "Ideal for junior engineers exploring:\n",
    "- Web scraping with requests and BeautifulSoup\n",
    "- Link extraction\n",
    "- HTTP file downloads\n",
    "\"\"\"\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import random  # Added for optional sampling\n",
    "\n",
    "# NIST Publications URL\n",
    "nist_url = \"https://csrc.nist.gov/publications/sp\"\n",
    "\n",
    "# Optional toggle for limiting sample size\n",
    "ENABLE_SAMPLE_MODE = True\n",
    "SAMPLE_SIZE = 20  # Reduce this number during testing\n",
    "\n",
    "# Step 1: Fetch the main NIST SP800 publications page\n",
    "response = requests.get(nist_url)\n",
    "print(f\"Fetching URL: {nist_url}, Status Code: {response.status_code}\")\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Step 2: Extract publication page links\n",
    "publication_links = [\n",
    "    a['href'] for a in soup.find_all('a', href=True)\n",
    "    if 'pub-title-link' in a.get('id', '') or 'mobile-pub-title' in a.get('class', [])\n",
    "]\n",
    "print(f\"Found {len(publication_links)} publication links\")\n",
    "\n",
    "# Sample subset of publication links if testing\n",
    "if ENABLE_SAMPLE_MODE:\n",
    "    publication_links = random.sample(publication_links, min(SAMPLE_SIZE, len(publication_links)))\n",
    "    print(f\"Sampling only {len(publication_links)} links for testing\")\n",
    "\n",
    "# Step 3: Extract PDF links from each publication page\n",
    "pdf_links = []\n",
    "for link in publication_links:\n",
    "    publication_url = link if link.startswith('http') else f\"https://csrc.nist.gov{link}\"\n",
    "    print(f\"Fetching publication page: {publication_url}\")\n",
    "    pub_response = requests.get(publication_url)\n",
    "    print(f\"Status Code: {pub_response.status_code}\")\n",
    "    pub_soup = BeautifulSoup(pub_response.text, 'html.parser')\n",
    "\n",
    "    # Find all PDF links\n",
    "    pdf_links.extend([\n",
    "        a['href'] for a in pub_soup.find_all('a', href=True)\n",
    "        if a['href'].endswith('.pdf')\n",
    "    ])\n",
    "\n",
    "print(f\"Found {len(pdf_links)} PDF links\")\n",
    "\n",
    "# Step 4: Download each PDF\n",
    "for pdf in pdf_links:\n",
    "    filename = pdf.split(\"/\")[-1]\n",
    "    pdf_url = pdf if pdf.startswith('http') else f\"https://csrc.nist.gov{pdf}\"\n",
    "    print(f\"Downloading PDF: {pdf_url}\")\n",
    "    pdf_response = requests.get(pdf_url)\n",
    "    with open(filename, 'wb') as f:\n",
    "        f.write(pdf_response.content)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}